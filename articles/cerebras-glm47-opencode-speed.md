---
title: "大速度時代が来た。Cerebras × GLM-4.7が気になる"
emoji: "⚡"
type: "tech"
topics: ["cerebras", "opencode", "ai", "llm", "glm"]
published: true
---

## はじめに

> 業務自動化Pythonエンジニア。バイブコーディング歴1年 ≒ エンジニア歴。

### 「エージェント向け」、みんな同じこと言ってない？

コード生成、ツール駆動型エージェント、マルチターン推論に特化。

最近のLLM、みんなこう言ってる。

正直、**もう横並びだと思ってる**。

インターネットに繋がって同じ知識ソースを得て、ClaudeやChatGPTが売れてるからそこを目指して...みんな同じところを目指してる。「こう答えてほしい」が決まってる。

だからといって「モデルどれでもいい」とは思ってなかった。**俺たちのClaudeを信じろ**。

正直Sonnet 3.7くらいでも余裕で仕事できる（あるならOpus 4.5使うけどな！）。

Gemini 3が出ても、Codexが出ても、**Claudeさえ使っておけば**って感じだった。

### Windsurfのswe-1.5で確信した

去年の9月、Windsurfが出してきた**速度特化モデル**。あれは確かに速かった。

友人（AIマン）は「バイブが追いつかない」と言ってお気に入りにしてた。

私も試してみた。

**全然思った通り動かなかった。**

右チャットアレルギーが出てしまったし、ターミナル操作が下手だし。速いのに使えない。

ここで確信した。**ハーネス次第だ**と。

### ハーネスとは

コーディングエージェントは「モデルの賢さ」だけじゃない。

- ファイル操作
- ターミナル操作全般
- MCP統合
- エージェント機構（並列実行、サブエージェント）
- ルールファイル（CLAUDE.md等）

これらの**ハーネスが揃ってないと、いくら速くても実用にならない**。

Claude Codeに慣れた**ぬるま湯開発マン**としては、どうしてもここが大事。

### そんな時、OpenCodeが面白そうだと気づいた

OpenCodeはCerebras対応してる。

つまり、**使い慣れたハーネスのまま、爆速モデルを試せる**。

**Claude信者卒業か？**

...とまでは言わないけど、ちょっと試してみたくなった。

## Cerebrasとは

去年この記事を読んで感銘を受けた。

> NVIDIA　なぜお前が　至高の領域に踏み入れないのか　教えてやろう
> 小さく刻むからだ　繋がねばならぬからだ　汎用 GPU だからだ

参考: [NVIDIA を超える AI チップ Cerebras の全貌 - Zenn](https://zenn.dev/because02and/articles/cerebras-overview)

**一言で言うと：** NVIDIAがウェハーから小さく切り出してGPUを作るのに対し、Cerebrasは**ウェハー全体を1つの巨大チップにする**。ギャグみたいなアプローチだけど、これが速い。

## GLM-4.7とは

Z.aiから発表された最新モデル。

**スペック：**
- 355Bパラメータ（アクティブ32B）
- MITライセンス
- 開発者ベンチマーク（SWEbench、LiveCodeBench）でDeepSeek-V3.2を上回る

**特徴：**
- **インターリーブド思考**: 各アクション前に推論を実施
- **保持型思考**: 推論コンテキストがターン間で保持される
- **日本語も得意**: オープンモデルの中でもトップクラス

## GLM-4.7を使う方法

### 選択肢

| 方法 | 速度 | コスト | 備考 |
|------|------|--------|------|
| **OpenCode Zen版** | 普通 | 無料 | アカウント登録不要 |
| **Cerebras API** | 爆速 | 従量課金 | 1,000 tok/s |
| **ローカル** | 遅い | 90万円〜 | Mac Studio 256GB必要 |

### OpenCode Zen版（無料）

1. `opencode`コマンドで起動
2. `/models`で「GLM-4.7 OpenCode Zen」を選択
3. 無料で使える

参考: [OpenCodeとGLM 4.7で無課金コーディングエージェント体験](https://nowokay.hatenablog.com/entry/2024/12/29/opencode_glm47)

### Cerebras API経由（爆速）

OpenCodeはCerebrasを公式サポートしてる。

1. [Cerebras Cloud](https://cloud.cerebras.ai/)で**無料のAPIキー**を取得
2. `opencode auth login` を実行
3. プロバイダー選択で「Cerebras」を選ぶ
4. APIキーを入力
5. `/models` でモデル選択

参考: [Cerebras公式 - OpenCode連携ガイド](https://inference-docs.cerebras.ai/integrations/opencode)

## Cerebrasの料金体系

### OpenCode経由（Inference API）

| プラン | 料金 | 1日の上限 | レート制限 |
|--------|------|----------|-----------|
| **Free** | $0 | 1Mトークン | 10 req/min |
| **Developer** | $10〜 + 従量課金 | なし | 250 req/min |
| **Enterprise** | 要問合せ | - | 最高 |

**GLM-4.7の従量課金（Developer以上）：**
- 入力: $2.25/M tokens
- 出力: $2.75/M tokens

**Freeは1日1Mトークンまで無料。** それ以上使いたいならDeveloperで従量課金。

### Cerebras Code（専用ツール）

定額で使いたいなら**Cerebras Code**（$50/月〜）を使う必要がある。OpenCode経由だと定額プランはない。

---

**結論：** OpenCodeで試すならまずFreeから。1日1Mトークンで足りなければDeveloper。

## まとめ

**大速度時代はすぐそこ。**

大事なのは**ハーネスの中で使えるかどうか**。

**OpenCode × 大速度**が勝利の鍵なのかも。

### 実際に使うとしたら？

oh-my-opencodeの構成で妄想すると：

| エージェント | モデル | 役割 |
|-------------|--------|------|
| Planner-Sisyphus | Opus 4.5 | 設計・計画 |
| Sisyphus | GLM-4.7 | 実装（爆速） |

**賢いClaudeが設計して、速いGLM-4.7が実装する。**

この使い分け、最高に楽しそう。検証が楽しみ。

## 参考リンク

- [GLM-4.7 - Cerebras公式ブログ](https://www.cerebras.ai/blog/glm-4-7)
- [GLM-4.7で自宅コーディングエージェントが現実的に - きしだのHatena](https://nowokay.hatenablog.com/entry/2025/01/06/glm47)
- [OpenCodeとGLM 4.7で無課金コーディングエージェント体験 - きしだのHatena](https://nowokay.hatenablog.com/entry/2024/12/29/opencode_glm47)
- [OpenCode 公式](https://opencode.ai/)
- [Cerebras 公式](https://www.cerebras.ai/)
